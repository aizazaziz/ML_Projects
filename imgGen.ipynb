{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP1V6s23kACuvVKx9W/iVVS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aizazaziz/ML_Projects/blob/main/imgGen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, cv2\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as T\n",
        "from torchvision.models.segmentation import deeplabv3_resnet50\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "gkjEhisu_kX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PairedImageDataset(Dataset):\n",
        "    def __init__(self, root):\n",
        "        self.input_dir = os.path.join(root, \"input\")\n",
        "        self.target_dir = os.path.join(root, \"target\")\n",
        "        self.files = os.listdir(self.input_dir)\n",
        "\n",
        "        self.transform = T.Compose([\n",
        "            T.Resize((256, 256)), # Further reduced image size\n",
        "            T.ToTensor()\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        name = self.files[idx]\n",
        "        inp = Image.open(os.path.join(self.input_dir, name)).convert(\"RGB\")\n",
        "        tar = Image.open(os.path.join(self.target_dir, name)).convert(\"RGB\")\n",
        "\n",
        "        return self.transform(inp), self.transform(tar)"
      ],
      "metadata": {
        "id": "oTj8t4AN_ku7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UNetBlock(nn.Module):\n",
        "    def __init__(self, in_c, out_c):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(in_c, out_c, 3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_c, out_c, 3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.down1 = UNetBlock(3, 64)\n",
        "        self.pool = nn.MaxPool2d(2)\n",
        "        self.down2 = UNetBlock(64,128)\n",
        "        self.down3 = UNetBlock(128,256)\n",
        "\n",
        "        self.bottleneck = UNetBlock(256,512)\n",
        "\n",
        "        self.up1 = nn.ConvTranspose2d(512,256,2,2)\n",
        "        self.dec1 = UNetBlock(512,256)\n",
        "\n",
        "        self.up2 = nn.ConvTranspose2d(256,128,2,2)\n",
        "        self.dec2 = UNetBlock(256,128)\n",
        "\n",
        "        self.up3 = nn.ConvTranspose2d(128,64,2,2)\n",
        "        self.dec3 = UNetBlock(128,64)\n",
        "\n",
        "        self.final = nn.Conv2d(64,3,1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        d1 = self.down1(x)\n",
        "        d2 = self.down2(self.pool(d1))\n",
        "        d3 = self.down3(self.pool(d2))\n",
        "\n",
        "        bn = self.bottleneck(self.pool(d3))\n",
        "\n",
        "        u1 = self.up1(bn)\n",
        "        u1 = self.dec1(torch.cat([u1,d3],1))\n",
        "\n",
        "        u2 = self.up2(u1)\n",
        "        u2 = self.dec2(torch.cat([u2,d2],1))\n",
        "\n",
        "        u3 = self.up3(u2)\n",
        "        u3 = self.dec3(torch.cat([u3,d1],1))\n",
        "\n",
        "        return torch.sigmoid(self.final(u3))\n"
      ],
      "metadata": {
        "id": "ifIj_UZX_kyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = UNet().to(DEVICE)\n",
        "\n",
        "dataset = PairedImageDataset(\"dataset\")   # <-- change path\n",
        "loader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "criterion = nn.L1Loss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=2e-4)"
      ],
      "metadata": {
        "id": "GKpuMe-RCgsG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 50\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    loop = tqdm(loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
        "    for inp, tar in loop:\n",
        "        inp, tar = inp.to(DEVICE), tar.to(DEVICE)\n",
        "\n",
        "        pred = model(inp)\n",
        "        loss = criterion(pred, tar)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        loop.set_postfix(loss=float(loss))\n",
        "\n",
        "torch.save(model.state_dict(), \"model.pth\")\n",
        "print(\"Training complete. Model saved.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTUZot6LClV8",
        "outputId": "d67f0f31-cc19-4db3-94af-90232cd90dfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/50:   0%|          | 0/17 [00:00<?, ?it/s]/tmp/ipython-input-1470285855.py:15: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.\n",
            "Consider using tensor.detach() first. (Triggered internally at /pytorch/torch/csrc/autograd/generated/python_variable_methods.cpp:836.)\n",
            "  loop.set_postfix(loss=float(loss))\n",
            "Epoch 1/50: 100%|██████████| 17/17 [00:02<00:00,  7.54it/s, loss=0.231]\n",
            "Epoch 2/50: 100%|██████████| 17/17 [00:00<00:00, 20.13it/s, loss=0.261]\n",
            "Epoch 3/50: 100%|██████████| 17/17 [00:00<00:00, 20.03it/s, loss=0.22]\n",
            "Epoch 4/50: 100%|██████████| 17/17 [00:00<00:00, 20.33it/s, loss=0.0923]\n",
            "Epoch 5/50: 100%|██████████| 17/17 [00:00<00:00, 20.30it/s, loss=0.0471]\n",
            "Epoch 6/50: 100%|██████████| 17/17 [00:00<00:00, 20.21it/s, loss=0.0595]\n",
            "Epoch 7/50: 100%|██████████| 17/17 [00:00<00:00, 19.74it/s, loss=0.0641]\n",
            "Epoch 8/50: 100%|██████████| 17/17 [00:00<00:00, 19.26it/s, loss=0.0291]\n",
            "Epoch 9/50: 100%|██████████| 17/17 [00:00<00:00, 19.04it/s, loss=0.0838]\n",
            "Epoch 10/50: 100%|██████████| 17/17 [00:00<00:00, 19.15it/s, loss=0.0574]\n",
            "Epoch 11/50: 100%|██████████| 17/17 [00:00<00:00, 20.27it/s, loss=0.0561]\n",
            "Epoch 12/50: 100%|██████████| 17/17 [00:00<00:00, 20.23it/s, loss=0.0523]\n",
            "Epoch 13/50: 100%|██████████| 17/17 [00:00<00:00, 20.19it/s, loss=0.0543]\n",
            "Epoch 14/50: 100%|██████████| 17/17 [00:00<00:00, 20.12it/s, loss=0.0496]\n",
            "Epoch 15/50: 100%|██████████| 17/17 [00:00<00:00, 19.89it/s, loss=0.0428]\n",
            "Epoch 16/50: 100%|██████████| 17/17 [00:00<00:00, 20.20it/s, loss=0.0425]\n",
            "Epoch 17/50: 100%|██████████| 17/17 [00:00<00:00, 20.39it/s, loss=0.0469]\n",
            "Epoch 18/50: 100%|██████████| 17/17 [00:00<00:00, 20.20it/s, loss=0.0417]\n",
            "Epoch 19/50: 100%|██████████| 17/17 [00:00<00:00, 19.75it/s, loss=0.0427]\n",
            "Epoch 20/50: 100%|██████████| 17/17 [00:00<00:00, 20.01it/s, loss=0.0377]\n",
            "Epoch 21/50: 100%|██████████| 17/17 [00:00<00:00, 20.15it/s, loss=0.0366]\n",
            "Epoch 22/50: 100%|██████████| 17/17 [00:00<00:00, 19.41it/s, loss=0.0387]\n",
            "Epoch 23/50: 100%|██████████| 17/17 [00:00<00:00, 18.75it/s, loss=0.0448]\n",
            "Epoch 24/50: 100%|██████████| 17/17 [00:00<00:00, 18.51it/s, loss=0.057]\n",
            "Epoch 25/50: 100%|██████████| 17/17 [00:00<00:00, 19.58it/s, loss=0.0298]\n",
            "Epoch 26/50: 100%|██████████| 17/17 [00:00<00:00, 19.95it/s, loss=0.0294]\n",
            "Epoch 27/50: 100%|██████████| 17/17 [00:00<00:00, 20.03it/s, loss=0.0372]\n",
            "Epoch 28/50: 100%|██████████| 17/17 [00:00<00:00, 19.94it/s, loss=0.0428]\n",
            "Epoch 29/50: 100%|██████████| 17/17 [00:00<00:00, 20.11it/s, loss=0.026]\n",
            "Epoch 30/50: 100%|██████████| 17/17 [00:00<00:00, 19.83it/s, loss=0.042]\n",
            "Epoch 31/50: 100%|██████████| 17/17 [00:00<00:00, 19.97it/s, loss=0.0316]\n",
            "Epoch 32/50: 100%|██████████| 17/17 [00:00<00:00, 20.07it/s, loss=0.0363]\n",
            "Epoch 33/50: 100%|██████████| 17/17 [00:01<00:00, 16.89it/s, loss=0.0338]\n",
            "Epoch 34/50: 100%|██████████| 17/17 [00:00<00:00, 17.32it/s, loss=0.0368]\n",
            "Epoch 35/50: 100%|██████████| 17/17 [00:00<00:00, 19.82it/s, loss=0.0296]\n",
            "Epoch 36/50: 100%|██████████| 17/17 [00:00<00:00, 19.46it/s, loss=0.0263]\n",
            "Epoch 37/50: 100%|██████████| 17/17 [00:00<00:00, 18.72it/s, loss=0.0335]\n",
            "Epoch 38/50: 100%|██████████| 17/17 [00:00<00:00, 18.31it/s, loss=0.0353]\n",
            "Epoch 39/50: 100%|██████████| 17/17 [00:00<00:00, 19.54it/s, loss=0.0316]\n",
            "Epoch 40/50: 100%|██████████| 17/17 [00:00<00:00, 19.63it/s, loss=0.0285]\n",
            "Epoch 41/50: 100%|██████████| 17/17 [00:00<00:00, 19.76it/s, loss=0.0295]\n",
            "Epoch 42/50: 100%|██████████| 17/17 [00:00<00:00, 19.73it/s, loss=0.0335]\n",
            "Epoch 43/50: 100%|██████████| 17/17 [00:00<00:00, 19.87it/s, loss=0.0507]\n",
            "Epoch 44/50: 100%|██████████| 17/17 [00:00<00:00, 19.71it/s, loss=0.0277]\n",
            "Epoch 45/50: 100%|██████████| 17/17 [00:00<00:00, 19.72it/s, loss=0.037]\n",
            "Epoch 46/50: 100%|██████████| 17/17 [00:00<00:00, 19.67it/s, loss=0.0381]\n",
            "Epoch 47/50: 100%|██████████| 17/17 [00:00<00:00, 19.94it/s, loss=0.0504]\n",
            "Epoch 48/50: 100%|██████████| 17/17 [00:00<00:00, 19.79it/s, loss=0.0339]\n",
            "Epoch 49/50: 100%|██████████| 17/17 [00:00<00:00, 19.64it/s, loss=0.0225]\n",
            "Epoch 50/50: 100%|██████████| 17/17 [00:00<00:00, 19.53it/s, loss=0.0287]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training complete. Model saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "model = UNet().to(DEVICE)\n",
        "model.load_state_dict(torch.load(\"model.pth\"))\n",
        "model.eval()\n",
        "\n",
        "transform = T.Compose([T.Resize((256,256)), T.ToTensor()])\n",
        "\n",
        "os.makedirs(\"results\", exist_ok=True)\n",
        "\n",
        "test_images = os.listdir(\"test\")\n",
        "\n",
        "for name in test_images:\n",
        "    img = Image.open(f\"test/{name}\").convert(\"RGB\")\n",
        "    inp = transform(img).unsqueeze(0).to(DEVICE)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out = model(inp)[0].cpu().permute(1,2,0).numpy()\n",
        "\n",
        "    out = (out*255).astype(\"uint8\")\n",
        "    cv2.imwrite(f\"results/{name}\", cv2.cvtColor(out, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "print(\"Done! Check results/ folder.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-cKEIpioCpSw",
        "outputId": "2e6008da-ea2f-4d4a-8ba7-f8bfb0eebd7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done! Check results/ folder.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "model_version = \"1\"\n",
        "output_dir = os.path.join(\"/content\", \"model_versions\")\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "torch.save(model.state_dict(), os.path.join(output_dir, f\"model_{model_version}.pth\"))\n",
        "print(f\"Model saved to {os.path.join(output_dir, f'model_{model_version}.pth')}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MeQIB3r1cNDt",
        "outputId": "6593e66d-5a46-4889-f45c-4fba4e774e56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to /content/model_versions/model_1.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "filename = 'model.pkl'\n",
        "pickle.dump(model, open(filename, 'wb'))"
      ],
      "metadata": {
        "id": "bjTQMQHCeYzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.serialization import load\n",
        "loaded_model = pickle.load(open(filename, 'rb'))"
      ],
      "metadata": {
        "id": "wQ4jB3yyer4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit\n",
        "import streamlit as st\n",
        "import os, cv2\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as T\n",
        "from torchvision.models.segmentation import deeplabv3_resnet50\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "class PairedImageDataset(Dataset):\n",
        "    def __init__(self, root):\n",
        "        self.input_dir = os.path.join(root, \"input\")\n",
        "        self.target_dir = os.path.join(root, \"target\")\n",
        "        self.files = os.listdir(self.input_dir)\n",
        "\n",
        "        self.transform = T.Compose([\n",
        "            T.Resize((256, 256)), # Further reduced image size\n",
        "            T.ToTensor()\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        name = self.files[idx]\n",
        "        inp = Image.open(os.path.join(self.input_dir, name)).convert(\"RGB\")\n",
        "        tar = Image.open(os.path.join(self.target_dir, name)).convert(\"RGB\")\n",
        "\n",
        "        return self.transform(inp), self.transform(tar)\n",
        "class UNetBlock(nn.Module):\n",
        "    def __init__(self, in_c, out_c):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(in_c, out_c, 3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_c, out_c, 3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.down1 = UNetBlock(3, 64)\n",
        "        self.pool = nn.MaxPool2d(2)\n",
        "        self.down2 = UNetBlock(64,128)\n",
        "        self.down3 = UNetBlock(128,256)\n",
        "\n",
        "        self.bottleneck = UNetBlock(256,512)\n",
        "\n",
        "        self.up1 = nn.ConvTranspose2d(512,256,2,2)\n",
        "        self.dec1 = UNetBlock(512,256)\n",
        "\n",
        "        self.up2 = nn.ConvTranspose2d(256,128,2,2)\n",
        "        self.dec2 = UNetBlock(256,128)\n",
        "\n",
        "        self.up3 = nn.ConvTranspose2d(128,64,2,2)\n",
        "        self.dec3 = UNetBlock(128,64)\n",
        "\n",
        "        self.final = nn.Conv2d(64,3,1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        d1 = self.down1(x)\n",
        "        d2 = self.down2(self.pool(d1))\n",
        "        d3 = self.down3(self.pool(d2))\n",
        "\n",
        "        bn = self.bottleneck(self.pool(d3))\n",
        "\n",
        "        u1 = self.up1(bn)\n",
        "        u1 = self.dec1(torch.cat([u1,d3],1))\n",
        "\n",
        "        u2 = self.up2(u1)\n",
        "        u2 = self.dec2(torch.cat([u2,d2],1))\n",
        "\n",
        "        u3 = self.up3(u2)\n",
        "        u3 = self.dec3(torch.cat([u3,d1],1))\n",
        "\n",
        "        return torch.sigmoid(self.final(u3))\n",
        "\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = UNet().to(DEVICE)\n",
        "\n",
        "dataset = PairedImageDataset(\"dataset\")   # <-- change path\n",
        "loader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "criterion = nn.L1Loss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=2e-4)\n",
        "EPOCHS = 2\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    loop = tqdm(loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
        "    for inp, tar in loop:\n",
        "        inp, tar = inp.to(DEVICE), tar.to(DEVICE)\n",
        "\n",
        "        pred = model(inp)\n",
        "        loss = criterion(pred, tar)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        loop.set_postfix(loss=loss.item())\n",
        "\n",
        "torch.save(model.state_dict(), \"model.pth\")\n",
        "print(\"Training complete. Model saved.\")\n",
        "\n",
        "\n",
        "model = UNet().to(DEVICE)\n",
        "model.load_state_dict(torch.load(\"model.pth\"))\n",
        "model.eval()\n",
        "\n",
        "transform = T.Compose([T.Resize((256,256)), T.ToTensor()])\n",
        "\n",
        "os.makedirs(\"results\", exist_ok=True)\n",
        "\n",
        "test_images = os.listdir(\"test\")\n",
        "\n",
        "for name in test_images:\n",
        "    img = Image.open(f\"test/{name}\").convert(\"RGB\")\n",
        "    inp = transform(img).unsqueeze(0).to(DEVICE)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out = model(inp)[0].cpu().permute(1,2,0).numpy()\n",
        "\n",
        "    out = (out*255).astype(\"uint8\")\n",
        "    cv2.imwrite(f\"results/{name}\", cv2.cvtColor(out, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "print(\"Done! Check results/ folder.\")\n",
        "\n",
        "# -----------------------------\n",
        "# Load MODELS\n",
        "# -----------------------------\n",
        "@st.cache_resource\n",
        "def load_sr_model():\n",
        "    model = UNet() # Instantiate the UNet model\n",
        "    model.load_state_dict(torch.load(\"model.pth\", map_location=\"cpu\")) # Load the state_dict\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "@st.cache_resource\n",
        "def load_enhance_model():\n",
        "    model = UNet() # Instantiate the UNet model\n",
        "    model.load_state_dict(torch.load(\"model.pth\", map_location=\"cpu\")) # Load the state_dict\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "sr_model = load_sr_model()\n",
        "enhance_model = load_enhance_model()\n",
        "\n",
        "# -----------------------------\n",
        "# Preprocessing & Postprocessing\n",
        "# -----------------------------\n",
        "transform = T.Compose([\n",
        "    T.ToTensor(),\n",
        "])\n",
        "\n",
        "def tensor_to_img(tensor):\n",
        "    arr = (tensor.detach().cpu().numpy().transpose(1, 2, 0) * 255).clip(0,255).astype(np.uint8)\n",
        "    return Image.fromarray(arr)\n",
        "\n",
        "# -----------------------------\n",
        "# SR + ENHANCEMENT PIPELINE\n",
        "# -----------------------------\n",
        "def enhance_pipeline(img, brightness, sharpness):\n",
        "    x = transform(img).unsqueeze(0)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Super Resolution first\n",
        "        sr = sr_model(x)\n",
        "\n",
        "        # Apply enhancement network\n",
        "        enhanced = enhance_model(sr)\n",
        "\n",
        "    out = tensor_to_img(enhanced.squeeze(0))\n",
        "\n",
        "    # Apply user-controlled adjustments\n",
        "    from PIL import ImageEnhance\n",
        "\n",
        "    if brightness != 1.0:\n",
        "        out = ImageEnhance.Brightness(out).enhance(brightness)\n",
        "\n",
        "    if sharpness != 1.0:\n",
        "        out = ImageEnhance.Sharpness(out).enhance(sharpness)\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# UI\n",
        "# -----------------------------\n",
        "st.title(\"Real Estate Image Enhancer (SR + HDR)\")\n",
        "\n",
        "uploaded = st.file_uploader(\"Upload an image\", type=['jpg', 'jpeg', 'png'])\n",
        "\n",
        "brightness = st.slider(\"Brightness\", 0.2, 2.0, 1.0, 0.05)\n",
        "sharpness = st.slider(\"Sharpness\", 0.2, 3.0, 1.0, 0.05)\n",
        "\n",
        "if uploaded:\n",
        "    img = Image.open(uploaded).convert(\"RGB\")\n",
        "\n",
        "    st.image(img, caption=\"Original\", use_column_width=True)\n",
        "\n",
        "    if st.button(\"Enhance Image\"):\n",
        "        out = enhance_pipeline(img, brightness, sharpness)\n",
        "        st.image(out, caption=\"Enhanced Output\", use_column_width=True)"
      ],
      "metadata": {
        "id": "XXn2hN3Vb2MZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 995
        },
        "outputId": "0afdd6db-5265-4865-a97f-86fe9cb1f279"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.12/dist-packages (1.51.0)\n",
            "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<6,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.2.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.3.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<13,>=7.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow<22,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.32.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (9.1.2)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (4.15.0)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.12/dist-packages (from streamlit) (3.1.45)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.5.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (4.25.1)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2.12.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2025.11.12)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.0.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.29.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'dataset/input'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1471778715.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPairedImageDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dataset\"\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# <-- change path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1471778715.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"input\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"target\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         self.transform = T.Compose([\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dataset/input'"
          ]
        }
      ]
    }
  ]
}